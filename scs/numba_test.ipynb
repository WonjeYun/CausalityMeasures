{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ols_gc\n",
    "import pandas as pd\n",
    "from calculation import LaggedTimeSeriesDF\n",
    "# from Statistical_Test import significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrangerCausality():\n",
    "    def __init__(self, lts_array, endog, exog, lag = None):\n",
    "        ''' input LaggedTimeSeriesDF object as lts_df'''\n",
    "        self.array = lts_array\n",
    "        self.endog = endog  # Dependent Variable Y\n",
    "        self.exog = exog  # Independent Variable X\n",
    "        self.lag = lag\n",
    "\n",
    "    def Granger_Caus(self, df=None, n_shuffles=0):\n",
    "        ## Prepare lists for storing results\n",
    "        granger_causalities = [0,0]\n",
    "        GCs = []\n",
    "        shuffled_TEs = []\n",
    "        p_values = []\n",
    "        z_scores = []\n",
    "\n",
    "        df = np.copy(self.array)\n",
    "\n",
    "        ## Require us to compare information transfer bidirectionally\n",
    "        for i in range(2):\n",
    "            ## Calculate Residuals after OLS Fitting, for both Independent and Joint Cases\n",
    "            joint_residuals = ols_gc.ols_res_cal(df[:, i], df[:, [i+2, 3-i]])\n",
    "            independent_residuals = ols_gc.ols_res_cal(df[:, i], df[:, i+2].reshape(-1, 1))\n",
    "\n",
    "            ## Use Geweke's formula for Granger Causality \n",
    "            granger_causalities[i] = ols_gc.granger_cal(independent_residuals, joint_residuals)\n",
    "\n",
    "        GCs.append(granger_causalities)\n",
    "        ## Calculate Significance of GC during this window\n",
    "        if n_shuffles > 0:\n",
    "            p, z, TE_mean = significance(df=df,\n",
    "                                            TE=granger_causalities,\n",
    "                                            endog=self.endog,\n",
    "                                            exog=self.exog,\n",
    "                                            lag=self.lag,\n",
    "                                            n_shuffles=n_shuffles,\n",
    "                                            method='granger_causality')\n",
    "\n",
    "            shuffled_TEs.append(TE_mean)\n",
    "            p_values.append(p)\n",
    "            z_scores.append(z)\n",
    "            # column are [XY, YX]\n",
    "            # rows are [TE, p_value, z_score, shuffled_TE]\n",
    "            self.results = np.concatenate(\n",
    "                (np.array(GCs), np.array(p_values), np.array(z_scores), np.array(shuffled_TEs)), axis=0)\n",
    "        else:\n",
    "            ## Store Granger Causality from X(t)->Y(t) and from Y(t)->X(t)\n",
    "            self.results = np.array(GCs)\n",
    "\n",
    "        return self.results\n",
    "    \n",
    "\n",
    "def significance(df, TE, endog, exog, lag, n_shuffles, method, bandwidth=None):\n",
    "    \"\"\"\n",
    "        Perform significance analysis on the hypothesis test of statistical causality, for both X(t)->Y(t)\n",
    "        and Y(t)->X(t) directions\n",
    "   \n",
    "        Calculated using:  Assuming stationarity, we shuffle the time series to provide the null hypothesis. \n",
    "                           The proportion of tests where TE > TE_shuffled gives the p-value significance level.\n",
    "                           The amount by which the calculated TE is greater than the average shuffled TE, divided\n",
    "                           by the standard deviation of the results, is the z-score significance level.\n",
    "\n",
    "        Arguments:\n",
    "            TE              -      (list)    Contains the transfer entropy in each direction, i.e. [TE_XY, TE_YX]\n",
    "            endog           -      (string)  The endogenous variable in the TE analysis being significance tested (i.e. X or Y) \n",
    "            exog            -      (string)  The exogenous variable in the TE analysis being significance tested (i.e. X or Y) \n",
    "            pdf_estimator   -      (string)  The pdf_estimator used in the original TE analysis\n",
    "            bins            -      (Dict of lists)  The bins used in the original TE analysis\n",
    "\n",
    "            n_shuffles      -      (float) Number of times to shuffle the dataframe, destroyig temporality\n",
    "            both            -      (Bool) Whether to shuffle both endog and exog variables (z-score) or just exog                                  variables (giving z*-score)  \n",
    "        Returns:\n",
    "            p_value         -      Probablity of observing the result given the null hypothesis\n",
    "            z_score         -      Number of Standard Deviations result is from mean (normalised)\n",
    "        \"\"\"\n",
    "\n",
    "    ## Prepare array for Transfer Entropy of each Shuffle\n",
    "    shuffled_TEs = np.zeros(shape=(2, n_shuffles))\n",
    "\n",
    "    for i in range(n_shuffles):\n",
    "        ## Perform Shuffle\n",
    "        df = shuffle_along_axis(df, axis=0)\n",
    "\n",
    "        if method == 'granger_causality':\n",
    "            ## Calculate New TE\n",
    "            shuffled_causality = GrangerCausality(df, endog=endog, exog=exog, lag=lag)\n",
    "            TE_shuffled = shuffled_causality.Granger_Caus(df, n_shuffles=0)\n",
    "\n",
    "    ## Calculate p-values for each direction\n",
    "    p_values = (np.count_nonzero(TE[0] < shuffled_TEs[0, :]) / n_shuffles, \\\n",
    "                np.count_nonzero(TE[1] < shuffled_TEs[1, :]) / n_shuffles)\n",
    "\n",
    "    shuff_te_zero = np.std(shuffled_TEs[0, :]) + np.finfo(float).eps\n",
    "    shuff_te_one = np.std(shuffled_TEs[1, :]) + np.finfo(float).eps\n",
    "\n",
    "    ## Calculate z-scores for each direction\n",
    "    z_scores = ((TE[0] - np.mean(shuffled_TEs[0, :])) / shuff_te_zero, \\\n",
    "                (TE[1] - np.mean(shuffled_TEs[1, :])) / shuff_te_one)\n",
    "\n",
    "    TE_mean = (np.mean(shuffled_TEs[0, :]), \\\n",
    "               np.mean(shuffled_TEs[1, :]))\n",
    "\n",
    "    ## Return the self.DF value to the unshuffled case\n",
    "    return p_values, z_scores, TE_mean\n",
    "\n",
    "def shuffle_along_axis(a, axis):\n",
    "    idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
    "    return np.take_along_axis(a, idx, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../PyCausality/Testing/Test_Utils/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.054181</td>\n",
       "      <td>0.306740</td>\n",
       "      <td>5.177009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/01/2018</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.038264</td>\n",
       "      <td>0.423752</td>\n",
       "      <td>6.641647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03/01/2018</td>\n",
       "      <td>64</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.451472</td>\n",
       "      <td>8.252325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/01/2018</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.002890</td>\n",
       "      <td>0.470023</td>\n",
       "      <td>6.488850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05/01/2018</td>\n",
       "      <td>6</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.309367</td>\n",
       "      <td>3.046403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>20/06/2019</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.446053</td>\n",
       "      <td>3.931973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>21/06/2019</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.450318</td>\n",
       "      <td>7.164833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>22/06/2019</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.009153</td>\n",
       "      <td>0.329843</td>\n",
       "      <td>3.325823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>23/06/2019</td>\n",
       "      <td>71</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>0.320296</td>\n",
       "      <td>8.998681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>24/06/2019</td>\n",
       "      <td>41</td>\n",
       "      <td>-0.137054</td>\n",
       "      <td>0.290409</td>\n",
       "      <td>7.063219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  S1        S2        S3        S4\n",
       "0    01/01/2018  22 -0.054181  0.306740  5.177009\n",
       "1    02/01/2018  35 -0.038264  0.423752  6.641647\n",
       "2    03/01/2018  64  0.005967  0.451472  8.252325\n",
       "3    04/01/2018  40 -0.002890  0.470023  6.488850\n",
       "4    05/01/2018   6  0.004926  0.309367  3.046403\n",
       "..          ...  ..       ...       ...       ...\n",
       "535  20/06/2019  10  0.002566  0.446053  3.931973\n",
       "536  21/06/2019  44 -0.001896  0.450318  7.164833\n",
       "537  22/06/2019   9 -0.009153  0.329843  3.325823\n",
       "538  23/06/2019  71 -0.003754  0.320296  8.998681\n",
       "539  24/06/2019  41 -0.137054  0.290409  7.063219\n",
       "\n",
       "[540 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lts_df = LaggedTimeSeriesDF(test_data[['S2', 'S3']], lag=2).df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 µs ± 629 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit GrangerCausality(lts_df, 'S2', 'S1', lag=2).Granger_Caus()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macs30123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
