{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "from scipy import stats, linalg\n",
    "from six import callable, string_types\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from TransferEntropy import TransferEntropy, LaggedTimeSeries, AutoBins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./Testing/Test_Utils/test_data.csv')\n",
    "test_numpy = test_data.to_numpy()[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.054181</td>\n",
       "      <td>0.306740</td>\n",
       "      <td>5.177009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/01/2018</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.038264</td>\n",
       "      <td>0.423752</td>\n",
       "      <td>6.641647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03/01/2018</td>\n",
       "      <td>64</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.451472</td>\n",
       "      <td>8.252325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/01/2018</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.002890</td>\n",
       "      <td>0.470023</td>\n",
       "      <td>6.488850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05/01/2018</td>\n",
       "      <td>6</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.309367</td>\n",
       "      <td>3.046403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>20/06/2019</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.446053</td>\n",
       "      <td>3.931973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>21/06/2019</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.450318</td>\n",
       "      <td>7.164833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>22/06/2019</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.009153</td>\n",
       "      <td>0.329843</td>\n",
       "      <td>3.325823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>23/06/2019</td>\n",
       "      <td>71</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>0.320296</td>\n",
       "      <td>8.998681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>24/06/2019</td>\n",
       "      <td>41</td>\n",
       "      <td>-0.137054</td>\n",
       "      <td>0.290409</td>\n",
       "      <td>7.063219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  S1        S2        S3        S4\n",
       "0    01/01/2018  22 -0.054181  0.306740  5.177009\n",
       "1    02/01/2018  35 -0.038264  0.423752  6.641647\n",
       "2    03/01/2018  64  0.005967  0.451472  8.252325\n",
       "3    04/01/2018  40 -0.002890  0.470023  6.488850\n",
       "4    05/01/2018   6  0.004926  0.309367  3.046403\n",
       "..          ...  ..       ...       ...       ...\n",
       "535  20/06/2019  10  0.002566  0.446053  3.931973\n",
       "536  21/06/2019  44 -0.001896  0.450318  7.164833\n",
       "537  22/06/2019   9 -0.009153  0.329843  3.325823\n",
       "538  23/06/2019  71 -0.003754  0.320296  8.998681\n",
       "539  24/06/2019  41 -0.137054  0.290409  7.063219\n",
       "\n",
       "[540 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5h/9cd9pzvx7rx5k8ty8zvbqc380000gn/T/ipykernel_3995/3265419399.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [slice(dim.min(),dim.max(),N) for dimname, dim in test_data.iteritems()]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[slice('01/01/2018', '31/12/2018', (20+0j)),\n",
       " slice(0, 293, (20+0j)),\n",
       " slice(-0.212948, 0.215818, (20+0j)),\n",
       " slice(0.255755181, 0.482722312, (20+0j)),\n",
       " slice(0.013185229, 17.89226835, (20+0j))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = complex(20)\n",
    "[slice(dim.min(),dim.max(),N) for dimname, dim in test_data.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22, -0.054181, 0.306739592, 5.177008664],\n",
       "       [35, -0.038264, 0.423752082, 6.641647156],\n",
       "       [64, 0.005967, 0.451471786, 8.252324509],\n",
       "       ...,\n",
       "       [9, -0.009153, 0.329843413, 3.325822994],\n",
       "       [71, -0.003754, 0.320295918, 8.99868101],\n",
       "       [41, -0.137054, 0.290409148, 7.063219041]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[slice(0, 293, (20+0j)),\n",
       " slice(-0.212948, 0.215818, (20+0j)),\n",
       " slice(0.255755181, 0.482722312, (20+0j)),\n",
       " slice(0.013185229, 17.89226835, (20+0j))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = complex(20)\n",
    "[slice(dim.min(),dim.max(),N) for dim in test_numpy.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TE = TransferEntropy(   DF = test_data,\n",
    "                        endog = 'S2',     # Dependent Variable\n",
    "                        exog = 'S3',      # Independent Variable\n",
    "                        lag = 2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>date_lag2</th>\n",
       "      <th>S1_lag2</th>\n",
       "      <th>S2_lag2</th>\n",
       "      <th>S3_lag2</th>\n",
       "      <th>S4_lag2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03/01/2018</td>\n",
       "      <td>64</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.451472</td>\n",
       "      <td>8.252325</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.054181</td>\n",
       "      <td>0.306740</td>\n",
       "      <td>5.177009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/01/2018</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.002890</td>\n",
       "      <td>0.470023</td>\n",
       "      <td>6.488850</td>\n",
       "      <td>02/01/2018</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-0.038264</td>\n",
       "      <td>0.423752</td>\n",
       "      <td>6.641647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05/01/2018</td>\n",
       "      <td>6</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.309367</td>\n",
       "      <td>3.046403</td>\n",
       "      <td>03/01/2018</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.451472</td>\n",
       "      <td>8.252325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06/01/2018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.035660</td>\n",
       "      <td>0.338410</td>\n",
       "      <td>2.878061</td>\n",
       "      <td>04/01/2018</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-0.002890</td>\n",
       "      <td>0.470023</td>\n",
       "      <td>6.488850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07/01/2018</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.014597</td>\n",
       "      <td>0.388144</td>\n",
       "      <td>6.195224</td>\n",
       "      <td>05/01/2018</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.309367</td>\n",
       "      <td>3.046403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>20/06/2019</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.446053</td>\n",
       "      <td>3.931973</td>\n",
       "      <td>18/06/2019</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.339382</td>\n",
       "      <td>3.029323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>21/06/2019</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.450318</td>\n",
       "      <td>7.164833</td>\n",
       "      <td>19/06/2019</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.080539</td>\n",
       "      <td>0.431670</td>\n",
       "      <td>3.012577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>22/06/2019</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.009153</td>\n",
       "      <td>0.329843</td>\n",
       "      <td>3.325823</td>\n",
       "      <td>20/06/2019</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.446053</td>\n",
       "      <td>3.931973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>23/06/2019</td>\n",
       "      <td>71</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>0.320296</td>\n",
       "      <td>8.998681</td>\n",
       "      <td>21/06/2019</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.450318</td>\n",
       "      <td>7.164833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>24/06/2019</td>\n",
       "      <td>41</td>\n",
       "      <td>-0.137054</td>\n",
       "      <td>0.290409</td>\n",
       "      <td>7.063219</td>\n",
       "      <td>22/06/2019</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.009153</td>\n",
       "      <td>0.329843</td>\n",
       "      <td>3.325823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  S1        S2        S3        S4   date_lag2  S1_lag2  \\\n",
       "2    03/01/2018  64  0.005967  0.451472  8.252325  01/01/2018     22.0   \n",
       "3    04/01/2018  40 -0.002890  0.470023  6.488850  02/01/2018     35.0   \n",
       "4    05/01/2018   6  0.004926  0.309367  3.046403  03/01/2018     64.0   \n",
       "5    06/01/2018   5  0.035660  0.338410  2.878061  04/01/2018     40.0   \n",
       "6    07/01/2018  34 -0.014597  0.388144  6.195224  05/01/2018      6.0   \n",
       "..          ...  ..       ...       ...       ...         ...      ...   \n",
       "535  20/06/2019  10  0.002566  0.446053  3.931973  18/06/2019      5.0   \n",
       "536  21/06/2019  44 -0.001896  0.450318  7.164833  19/06/2019      5.0   \n",
       "537  22/06/2019   9 -0.009153  0.329843  3.325823  20/06/2019     10.0   \n",
       "538  23/06/2019  71 -0.003754  0.320296  8.998681  21/06/2019     44.0   \n",
       "539  24/06/2019  41 -0.137054  0.290409  7.063219  22/06/2019      9.0   \n",
       "\n",
       "      S2_lag2   S3_lag2   S4_lag2  \n",
       "2   -0.054181  0.306740  5.177009  \n",
       "3   -0.038264  0.423752  6.641647  \n",
       "4    0.005967  0.451472  8.252325  \n",
       "5   -0.002890  0.470023  6.488850  \n",
       "6    0.004926  0.309367  3.046403  \n",
       "..        ...       ...       ...  \n",
       "535  0.003326  0.339382  3.029323  \n",
       "536  0.080539  0.431670  3.012577  \n",
       "537  0.002566  0.446053  3.931973  \n",
       "538 -0.001896  0.450318  7.164833  \n",
       "539 -0.009153  0.329843  3.325823  \n",
       "\n",
       "[538 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TE.lts.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 s ± 35.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit TE.nonlinear_TE(pdf_estimator = 'kernel', n_shuffles=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip window strolling\n",
    "\n",
    "class LaggedTimeSeriesDF():\n",
    "    def __init__(self, df, lag=None):\n",
    "        self.df = df\n",
    "\n",
    "        if lag is not None:\n",
    "            self.t = lag\n",
    "            self.df = self.__apply_lags__()\n",
    "    \n",
    "    def __apply_lags__(self):\n",
    "        new_df = self.df.copy(deep=True).dropna()\n",
    "        col_names = self.df.columns.values.tolist()\n",
    "        # generate lagged array with t lags\n",
    "        for col_name in col_names:\n",
    "                new_df[col_name + '_lag' + str(self.t)] = self.df[col_name].shift(self.t)\n",
    "\n",
    "        return new_df.iloc[self.t:]\n",
    "\n",
    "class GrangerCausality():\n",
    "    def __init__(self, lts_array, endog, exog, lag = None):\n",
    "        ''' input LaggedTimeSeriesDF object as lts_df'''\n",
    "        self.array = lts_array\n",
    "        self.endog = endog  # Dependent Variable Y\n",
    "        self.exog = exog  # Independent Variable X\n",
    "        self.lag = lag\n",
    "\n",
    "    def Granger_Caus(self, df=None, n_shuffles=0):\n",
    "        ## Prepare lists for storing results\n",
    "        granger_causalities = [0,0]\n",
    "        GCs = []\n",
    "        shuffled_TEs = []\n",
    "        p_values = []\n",
    "        z_scores = []\n",
    "\n",
    "        df = np.copy(self.array)\n",
    "\n",
    "        ## Require us to compare information transfer bidirectionally\n",
    "        for i in range(2):\n",
    "            ## Calculate Residuals after OLS Fitting, for both Independent and Joint Cases\n",
    "            joint_residuals = self.ols_res_cal(df[:, i], df[:, [i+2, 3-i]])\n",
    "            independent_residuals = self.ols_res_cal(df[:, i], df[:, i+2].reshape(-1, 1))\n",
    "\n",
    "            ## Use Geweke's formula for Granger Causality \n",
    "            granger_causalities[i] = self.granger_causality(independent_residuals, joint_residuals)\n",
    "\n",
    "        GCs.append(granger_causalities)\n",
    "        ## Calculate Significance of GC during this window\n",
    "        if n_shuffles > 0:\n",
    "            p, z, TE_mean = significance(df=df,\n",
    "                                            TE=granger_causalities,\n",
    "                                            endog=self.endog,\n",
    "                                            exog=self.exog,\n",
    "                                            lag=self.lag,\n",
    "                                            n_shuffles=n_shuffles,\n",
    "                                            method='granger_causality')\n",
    "\n",
    "            shuffled_TEs.append(TE_mean)\n",
    "            p_values.append(p)\n",
    "            z_scores.append(z)\n",
    "            # column = [XY, YX]\n",
    "            # rows = [TE, p_value, z_score, shuffled_TE]\n",
    "            self.results = np.concatenate(\n",
    "                (np.array(GCs), np.array(p_values), np.array(z_scores), np.array(shuffled_TEs)), axis=0)\n",
    "        else:\n",
    "            ## Store Granger Causality from X(t)->Y(t) and from Y(t)->X(t)\n",
    "            self.results = np.array(GCs)\n",
    "\n",
    "        return self.results\n",
    "    \n",
    "    #@njit\n",
    "    def ols_res_cal(self, y, x):\n",
    "        x = np.append(np.ones((len(x), 1)), x, axis=1)\n",
    "        ins = np.linalg.inv(np.dot(x.T, x))\n",
    "        out = np.dot(x.T, y)\n",
    "        beta = np.dot(ins, out)\n",
    "        res = y - np.dot(x, beta)\n",
    "        return res\n",
    "    \n",
    "    #@njit\n",
    "    def granger_causality(self, independent_residuals, joint_residuals):\n",
    "        ind_res_var = np.var(independent_residuals) + np.finfo(float).eps\n",
    "        jnt_res_var = np.var(joint_residuals) + np.finfo(float).eps\n",
    "        gc =  np.log(ind_res_var / jnt_res_var)\n",
    "        return gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0042882572997716755, 0.2, 0.6763686505785762, 0.0022425761872889413],\n",
       " [0.001450396826136952, 0.45, 0.04280969182065394, 0.0013972101907776286]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lts_df = LaggedTimeSeriesDF(test_data[['S2', 'S3']], lag=2).df.to_numpy()\n",
    "GrangerCausality(lts_df, 'S2', 'S1', lag=2).Granger_Caus(n_shuffles=20).transpose().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S2_lag2</th>\n",
       "      <th>S3_lag2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.451472</td>\n",
       "      <td>-0.054181</td>\n",
       "      <td>0.306740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002890</td>\n",
       "      <td>0.470023</td>\n",
       "      <td>-0.038264</td>\n",
       "      <td>0.423752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.309367</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>0.451472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.035660</td>\n",
       "      <td>0.338410</td>\n",
       "      <td>-0.002890</td>\n",
       "      <td>0.470023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.014597</td>\n",
       "      <td>0.388144</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.309367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.446053</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.339382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.450318</td>\n",
       "      <td>0.080539</td>\n",
       "      <td>0.431670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>-0.009153</td>\n",
       "      <td>0.329843</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.446053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>-0.003754</td>\n",
       "      <td>0.320296</td>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.450318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>-0.137054</td>\n",
       "      <td>0.290409</td>\n",
       "      <td>-0.009153</td>\n",
       "      <td>0.329843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           S2        S3   S2_lag2   S3_lag2\n",
       "2    0.005967  0.451472 -0.054181  0.306740\n",
       "3   -0.002890  0.470023 -0.038264  0.423752\n",
       "4    0.004926  0.309367  0.005967  0.451472\n",
       "5    0.035660  0.338410 -0.002890  0.470023\n",
       "6   -0.014597  0.388144  0.004926  0.309367\n",
       "..        ...       ...       ...       ...\n",
       "535  0.002566  0.446053  0.003326  0.339382\n",
       "536 -0.001896  0.450318  0.080539  0.431670\n",
       "537 -0.009153  0.329843  0.002566  0.446053\n",
       "538 -0.003754  0.320296 -0.001896  0.450318\n",
       "539 -0.137054  0.290409 -0.009153  0.329843\n",
       "\n",
       "[538 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lts_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferEntropyArray():\n",
    "    def __init__(self, lts_array, endog, exog, lag = None, method=None):\n",
    "        ''' input LaggedTimeSeriesDF object to numpy array lts_array'''\n",
    "        self.array = lts_array\n",
    "        self.endog = endog  # Dependent Variable Y\n",
    "        self.exog = exog  # Independent Variable X\n",
    "        self.lag = lag\n",
    "\n",
    "        self.covars = self.covariance_cal(method)\n",
    "\n",
    "    # @njit\n",
    "    def covariance_cal(self, method):\n",
    "        self.covars = [[], []]\n",
    "        for i in range(2):\n",
    "\n",
    "            if method == 'mi':\n",
    "                self.covars[i] = [np.ones(shape=(1, 1)) * np.var(self.array[:, [1 - i]]),\n",
    "                                  np.ones(shape=(1, 1)) * np.var(self.array[:, [i]]),\n",
    "                                  np.cov(self.array[:, [1 - i, i]].T)]\n",
    "            else:\n",
    "                self.covars[i] = [np.cov(self.array[:, [i, i+2, 3-i]].T),\n",
    "                                  np.cov(self.array[:, [3-i, i+2]].T),\n",
    "                                  np.cov(self.array[:, [i, i+2]].T),\n",
    "                                  np.ones(shape=(1, 1)) * np.var(self.array[:, [i+2]])]\n",
    "        return self.covars\n",
    "    \n",
    "    def TransferEntropy(self, df=None, bandwidth=None, gridpoints=20, n_shuffles=0):\n",
    "        ## Prepare lists for storing results\n",
    "        TEs = []\n",
    "        shuffled_TEs = []\n",
    "        p_values = []\n",
    "        z_scores = []\n",
    "\n",
    "        df = np.copy(self.array)\n",
    "\n",
    "        ## Initialise list to return TEs\n",
    "        transfer_entropies = [0, 0]\n",
    "\n",
    "        ## Require us to compare information transfer bidirectionally\n",
    "        for i in range(2):\n",
    "            ### Entropy calculated using Probability Density Estimation:\n",
    "            ### Estimate PDF using Gaussian Kernels and use H(x) = p(x) log p(x)\n",
    "            ## 1. H(Y,Y-t,X-t)\n",
    "            H1 = self.get_entropy(df=df[:, [i, i+2, 3-i]],\n",
    "                                gridpoints=gridpoints,\n",
    "                                bandwidth=bandwidth,\n",
    "                                covar=self.covars[i][0])\n",
    "            ## 2. H(Y-t,X-t)\n",
    "            H2 = self.get_entropy(df=df[:, [3-i, i+2]],\n",
    "                                gridpoints=gridpoints,\n",
    "                                bandwidth=bandwidth,\n",
    "                                covar=self.covars[i][1])\n",
    "            ## 3. H(Y,Y-t)\n",
    "            H3 = self.get_entropy(df=df[:, [i, i+2]],\n",
    "                                gridpoints=gridpoints,\n",
    "                                bandwidth=bandwidth,\n",
    "                                covar=self.covars[i][2])\n",
    "            ## 4. H(Y-t)\n",
    "            H4 = self.get_entropy(df=df[:, [i+2]],\n",
    "                                gridpoints=gridpoints,\n",
    "                                bandwidth=bandwidth,\n",
    "                                covar=self.covars[i][3])\n",
    "\n",
    "            ### Calculate Conditonal Entropy using: H(Y|X-t,Y-t) = H(Y,X-t,Y-t) - H(X-t,Y-t)\n",
    "            conditional_entropy_joint = H1 - H2\n",
    "\n",
    "            ### And Conditional Entropy independent of X(t) H(Y|Y-t) = H(Y,Y-t) - H(Y-t)            \n",
    "            conditional_entropy_independent = H3 - H4\n",
    "\n",
    "            ### Directional Transfer Entropy is the difference between the conditional entropies\n",
    "            transfer_entropies[i] = conditional_entropy_independent - conditional_entropy_joint\n",
    "\n",
    "        TEs.append(transfer_entropies)\n",
    "\n",
    "        ## Calculate Significance of TE during this window\n",
    "        if n_shuffles > 0:\n",
    "            p, z, TE_mean = significance(df=df,\n",
    "                                            TE=transfer_entropies,\n",
    "                                            endog=self.endog,\n",
    "                                            exog=self.exog,\n",
    "                                            lag=self.lag,\n",
    "                                            n_shuffles=n_shuffles,\n",
    "                                            bandwidth=bandwidth,\n",
    "                                            method='transfer_entropy')\n",
    "\n",
    "            shuffled_TEs.append(TE_mean)\n",
    "            p_values.append(p)\n",
    "            z_scores.append(z)\n",
    "            ## Store Significance Transfer Entropy from X(t)->Y(t) and from Y(t)->X(t)\n",
    "            self.results = np.concatenate((np.array(TEs), np.array(p_values), np.array(z_scores), np.array(shuffled_TEs)), axis=0)\n",
    "        else:\n",
    "            ## Store Significance Transfer Entropy from X(t)->Y(t) and from Y(t)->X(t)\n",
    "            self.results = np.array(TEs)\n",
    "            \n",
    "        return self.results\n",
    "    \n",
    "    def MutualInformation(self, df=None, bandwidth=None, gridpoints=20, n_shuffles=0):\n",
    "        ## Prepare lists for storing results\n",
    "        Norm_MIs = []\n",
    "        shuffled_MIs = []\n",
    "        p_values = []\n",
    "        z_scores = []\n",
    "\n",
    "        df = np.copy(self.array)\n",
    "\n",
    "        ## Initialise list to return TEs\n",
    "        # mutual_informations = [0, 0]\n",
    "        normalized_mi = [0, 0]\n",
    "\n",
    "        ## Require us to compare information transfer bidirectionally\n",
    "        for i in range(2):\n",
    "            ### Entropy calculated using Probability Density Estimation:\n",
    "            ### Estimate PDF using Gaussian Kernels and use H(x) = p(x) log p(x)\n",
    "            ## 1. H(Y,Y-t,X-t)\n",
    "            H1 = self.get_entropy(df=df[:, [1 - i]],\n",
    "                                gridpoints=gridpoints,\n",
    "                                bandwidth=bandwidth,\n",
    "                                covar=self.covars[i][0])\n",
    "            ## 2. H(Y-t,X-t)\n",
    "            H2 = self.get_entropy(df=df[:, [i]],\n",
    "                                gridpoints=gridpoints,\n",
    "                                bandwidth=bandwidth,\n",
    "                                covar=self.covars[i][1])\n",
    "            ## 3. H(Y,Y-t)\n",
    "            H3 = self.get_entropy(df=df[:, [1 - i, i]],\n",
    "                                gridpoints=gridpoints,\n",
    "                                bandwidth=bandwidth,\n",
    "                                covar=self.covars[i][2])\n",
    "\n",
    "            normalized_mi[i] = 1 - (H1 + H2 - H3 / max(H1, H2))\n",
    "\n",
    "        Norm_MIs.append(normalized_mi)\n",
    "\n",
    "        ## Calculate Significance of TE during this window\n",
    "        if n_shuffles > 0:\n",
    "            p, z, MI_mean = significance(df=df,\n",
    "                                            TE=normalized_mi,\n",
    "                                            endog=self.endog,\n",
    "                                            exog=self.exog,\n",
    "                                            lag=self.lag,\n",
    "                                            n_shuffles=n_shuffles,\n",
    "                                            bandwidth=bandwidth,\n",
    "                                            method='mutual_information')\n",
    "\n",
    "            shuffled_MIs.append(MI_mean)\n",
    "            p_values.append(p)\n",
    "            z_scores.append(z)\n",
    "            ## Store Significance Transfer Entropy from X(t)->Y(t) and from Y(t)->X(t)\n",
    "            self.results = np.concatenate((np.array(Norm_MIs), np.array(p_values), np.array(z_scores), np.array(shuffled_MIs)), axis=0)\n",
    "        else:\n",
    "            ## Store Significance Transfer Entropy from X(t)->Y(t) and from Y(t)->X(t)\n",
    "            self.results = np.array(Norm_MIs)\n",
    "            \n",
    "        return self.results\n",
    "\n",
    "    def get_entropy(self, df, gridpoints=20, bandwidth=None, covar=None):\n",
    "        \"\"\"\n",
    "            Function for calculating entropy from a probability mass \n",
    "            \n",
    "        Args:\n",
    "            df          -       (DataFrame) Samples over which to estimate density\n",
    "            gridpoints  -       (int)       Number of gridpoints when integrating KDE over \n",
    "                                            the domain. Used if estimator='kernel'\n",
    "            bandwidth   -       (float)     Bandwidth for KDE (scalar multiple to covariance\n",
    "                                            matrix). Used if estimator='kernel'\n",
    "            estimator   -       (string)    'histogram' or 'kernel'\n",
    "            bins        -       (Dict of lists) Bin edges for NDHistogram. Used if estimator\n",
    "                                            = 'histogram'\n",
    "            covar       -       (Numpy ndarray) Covariance matrix between dimensions of df. \n",
    "                                            Used if estimator = 'kernel'\n",
    "        Returns:\n",
    "            entropy     -       (float)     Shannon entropy in bits\n",
    "\n",
    "        \"\"\"\n",
    "        # df is np.array\n",
    "        pdf = self.pdf_kde(df, gridpoints, bandwidth, covar)\n",
    "        # log base 2 returns H(X) in bits\n",
    "        return -np.sum(pdf * ma.log2(pdf).filled(0))\n",
    "    \n",
    "    def pdf_kde(self, df, gridpoints=None, bandwidth=1, covar=None):\n",
    "        \"\"\"\n",
    "            Function for non-parametric density estimation using Kernel Density Estimation\n",
    "\n",
    "        Args:\n",
    "            df          -       (DataFrame) Samples over which to estimate density\n",
    "            gridpoints  -       (int)       Number of gridpoints when integrating KDE over \n",
    "                                            the domain. Used if estimator='kernel'\n",
    "            bandwidth   -       (float)     Bandwidth for KDE (scalar multiple to covariance\n",
    "                                            matrix).\n",
    "            covar       -       (Numpy ndarray) Covariance matrix between dimensions of df. \n",
    "                                            If None, these are calculated from df during the \n",
    "                                            KDE analysis\n",
    "\n",
    "        Returns:\n",
    "            Z/Z.sum()   -       (Numpy ndarray) Probability of a sample being between\n",
    "                                            specific gridpoints (technically a probability mass)\n",
    "        \"\"\"\n",
    "        # df is np.array\n",
    "        ## Create Meshgrid to capture data\n",
    "        if gridpoints is None:\n",
    "            gridpoints = 20\n",
    "        \n",
    "        N = complex(gridpoints)\n",
    "        \n",
    "        slices = [slice(dim.min(),dim.max(),N) for dim in df.T]\n",
    "        grids = np.mgrid[slices]\n",
    "\n",
    "        ## Pass Meshgrid to Scipy Gaussian KDE to Estimate PDF\n",
    "        positions = np.vstack([X.ravel() for X in grids])\n",
    "        values = df.T\n",
    "        kernel = _kde_(values, bw_method=bandwidth, covar=covar)\n",
    "        Z = np.reshape(kernel(positions).T, grids[0].shape) \n",
    "\n",
    "        ## Normalise \n",
    "        return Z/Z.sum()\n",
    "\n",
    "\n",
    "class _kde_(stats.gaussian_kde):\n",
    "    \"\"\"\n",
    "    Subclass of scipy.stats.gaussian_kde. This is to enable the passage of a pre-defined covariance matrix, via the\n",
    "    `covar` parameter. This is handled internally within TransferEntropy class.\n",
    "    The matrix is calculated on the overall dataset, before windowing, which allows for consistency between windows,\n",
    "    and avoiding duplicative computational operations, compared with calculating the covariance each window.\n",
    "\n",
    "    Functions left as much as possible identical to scipi.stats.gaussian_kde; docs available:\n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, bw_method=None, df=None, covar=None):\n",
    "        self.dataset = np.atleast_2d(dataset)\n",
    "        if not self.dataset.size > 1:\n",
    "            raise ValueError(\"`dataset` input should have multiple elements.\")\n",
    "\n",
    "        self.d, self.n = self.dataset.shape\n",
    "        self.set_bandwidth(bw_method=bw_method, covar=covar)\n",
    "\n",
    "\n",
    "    def set_bandwidth(self, bw_method=None, covar=None):\n",
    "        \n",
    "        if bw_method is None:\n",
    "            pass\n",
    "        elif np.isscalar(bw_method) and not isinstance(bw_method, string_types):\n",
    "            self._bw_method = 'use constant'\n",
    "            self.covariance_factor = lambda: bw_method\n",
    "        else:\n",
    "            msg = \"`bw_method` should be 'scott', 'silverman', a scalar \" \\\n",
    "                  \"or a callable.\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self._compute_covariance(covar)\n",
    "\n",
    "    def _compute_covariance(self, covar):\n",
    "        \"\"\"Computes the covariance matrix for each Gaussian kernel using\n",
    "        covariance_factor().\n",
    "        \"\"\"\n",
    "        if covar is not None:\n",
    "            self._data_covariance = covar\n",
    "            self._data_inv_cov = linalg.inv(self._data_covariance)\n",
    "        \n",
    "        self.factor = self.covariance_factor()\n",
    "        # Cache covariance and Cholesky decomp of covariance\n",
    "        if not hasattr(self, '_data_cho_cov'):\n",
    "            self._data_covariance = np.atleast_2d(np.cov(self.dataset, rowvar=1,\n",
    "                                            bias=False))\n",
    "            self._data_cho_cov = linalg.cholesky(self._data_covariance,\n",
    "                                                lower=True)\n",
    "\n",
    "        self.covariance = self._data_covariance * self.factor**2\n",
    "        self.cho_cov = (self._data_cho_cov * self.factor).astype(np.float64)\n",
    "        self.log_det = 2*np.log(np.diag(self.cho_cov\n",
    "                                        * np.sqrt(2*np.pi))).sum()\n",
    "\n",
    "    @property\n",
    "    def inv_cov(self):\n",
    "        # Re-compute from scratch each time because I'm not sure how this is\n",
    "        # used in the wild. (Perhaps users change the `dataset`, since it's\n",
    "        # not a private attribute?) `_compute_covariance` used to recalculate\n",
    "        # all these, so we'll recalculate everything now that this is a\n",
    "        # a property.\n",
    "        self.factor = self.covariance_factor()\n",
    "        self._data_covariance = np.atleast_2d(np.cov(self.dataset, rowvar=1,\n",
    "                                        bias=False, aweights=self.weights))\n",
    "        return linalg.inv(self._data_covariance) / self.factor**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16 s ± 27.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit TransferEntropyArray(lts_df, 'S2', 'S1', lag=2).TransferEntropy(n_shuffles=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance(df, TE, endog, exog, lag, n_shuffles, method, bandwidth=None):\n",
    "    \"\"\"\n",
    "        Perform significance analysis on the hypothesis test of statistical causality, for both X(t)->Y(t)\n",
    "        and Y(t)->X(t) directions\n",
    "   \n",
    "        Calculated using:  Assuming stationarity, we shuffle the time series to provide the null hypothesis. \n",
    "                           The proportion of tests where TE > TE_shuffled gives the p-value significance level.\n",
    "                           The amount by which the calculated TE is greater than the average shuffled TE, divided\n",
    "                           by the standard deviation of the results, is the z-score significance level.\n",
    "\n",
    "        Arguments:\n",
    "            TE              -      (list)    Contains the transfer entropy in each direction, i.e. [TE_XY, TE_YX]\n",
    "            endog           -      (string)  The endogenous variable in the TE analysis being significance tested (i.e. X or Y) \n",
    "            exog            -      (string)  The exogenous variable in the TE analysis being significance tested (i.e. X or Y) \n",
    "            pdf_estimator   -      (string)  The pdf_estimator used in the original TE analysis\n",
    "            bins            -      (Dict of lists)  The bins used in the original TE analysis\n",
    "\n",
    "            n_shuffles      -      (float) Number of times to shuffle the dataframe, destroyig temporality\n",
    "            both            -      (Bool) Whether to shuffle both endog and exog variables (z-score) or just exog                                  variables (giving z*-score)  \n",
    "        Returns:\n",
    "            p_value         -      Probablity of observing the result given the null hypothesis\n",
    "            z_score         -      Number of Standard Deviations result is from mean (normalised)\n",
    "        \"\"\"\n",
    "\n",
    "    ## Prepare array for Transfer Entropy of each Shuffle\n",
    "    shuffled_TEs = np.zeros(shape=(2, n_shuffles))\n",
    "\n",
    "    for i in range(n_shuffles):\n",
    "        ## Perform Shuffle\n",
    "        df = shuffle_along_axis(df, axis=0)\n",
    "\n",
    "        if method == 'granger_causality':\n",
    "            ## Calculate New TE\n",
    "            shuffled_causality = GrangerCausality(df, endog=endog, exog=exog, lag=lag)\n",
    "            TE_shuffled = shuffled_causality.Granger_Caus(df, n_shuffles=0)\n",
    "        elif method == 'mutual information':\n",
    "            ## Calculate New TE\n",
    "            shuffled_causality = TransferEntropyArray(df, endog=endog, exog=exog, lag=lag, method = 'mi')\n",
    "            TE_shuffled = shuffled_causality.MutualInformation(df, bandwidth, n_shuffles=0)\n",
    "        else:\n",
    "            ## Calculate New TE\n",
    "            shuffled_causality = TransferEntropyArray(df, endog=endog, exog=exog, lag=lag)\n",
    "            TE_shuffled = shuffled_causality.TransferEntropy(df, bandwidth, n_shuffles=0)\n",
    "        shuffled_TEs[:, i] = TE_shuffled\n",
    "\n",
    "    ## Calculate p-values for each direction\n",
    "    p_values = (np.count_nonzero(TE[0] < shuffled_TEs[0, :]) / n_shuffles, \\\n",
    "                np.count_nonzero(TE[1] < shuffled_TEs[1, :]) / n_shuffles)\n",
    "\n",
    "    shuff_te_zero = np.std(shuffled_TEs[0, :]) + np.finfo(float).eps\n",
    "    shuff_te_one = np.std(shuffled_TEs[1, :]) + np.finfo(float).eps\n",
    "\n",
    "    ## Calculate z-scores for each direction\n",
    "    z_scores = ((TE[0] - np.mean(shuffled_TEs[0, :])) / shuff_te_zero, \\\n",
    "                (TE[1] - np.mean(shuffled_TEs[1, :])) / shuff_te_one)\n",
    "\n",
    "    TE_mean = (np.mean(shuffled_TEs[0, :]), \\\n",
    "               np.mean(shuffled_TEs[1, :]))\n",
    "\n",
    "    ## Return the self.DF value to the unshuffled case\n",
    "    return p_values, z_scores, TE_mean\n",
    "\n",
    "def shuffle_along_axis(a, axis):\n",
    "    idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
    "    return np.take_along_axis(a, idx, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macs30123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
